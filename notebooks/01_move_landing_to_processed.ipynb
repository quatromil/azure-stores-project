{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Conexi√≥n al Storage Gen2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-10-17T17:43:01.3896188Z",
              "execution_start_time": "2025-10-17T17:43:01.1853531Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "76b68cc9-d276-49f8-bc70-e8738cbf5832",
              "queued_time": "2025-10-17T17:43:01.0185749Z",
              "session_id": "52",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkpoolnew",
              "state": "finished",
              "statement_id": 5,
              "statement_ids": [
                5
              ]
            },
            "text/plain": [
              "StatementMeta(sparkpoolnew, 52, 5, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from azure.storage.filedatalake import DataLakeServiceClient\n",
        "import pytz\n",
        "\n",
        "# Configuraci√≥n de acceso:\n",
        "storage_account_name = \"adlsstoresproject\"\n",
        "storage_account_key = \"************\"\n",
        "\n",
        "# --- Conexi√≥n al Data Lake ---\n",
        "service_client = DataLakeServiceClient(\n",
        "    account_url=f\"https://{storage_account_name}.dfs.core.windows.net\",\n",
        "    credential=storage_account_key\n",
        ")\n",
        "\n",
        "# Variables\n",
        "container_name = \"datalake\"\n",
        "root_source_folder = \"landing\"\n",
        "root_processed_folder = \"processed\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Identificar archivos **sales.csv** y **customers.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-10-17T17:43:01.9735902Z",
              "execution_start_time": "2025-10-17T17:43:01.4000894Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "1dbdcad2-53d7-4170-a217-e202d39d2350",
              "queued_time": "2025-10-17T17:43:01.0708245Z",
              "session_id": "52",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkpoolnew",
              "state": "finished",
              "statement_id": 6,
              "statement_ids": [
                6
              ]
            },
            "text/plain": [
              "StatementMeta(sparkpoolnew, 52, 6, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Tiendas encontradas: ['store-a', 'store-b', 'store-c']\n",
            "\n",
            "üìÅ Archivos dentro de 'store-a':\n",
            " Customer - abfss://datalake@adlsstoresproject.dfs.core.windows.net/landing/store-a/customers.csv\n",
            " Sales - abfss://datalake@adlsstoresproject.dfs.core.windows.net/landing/store-a/sales.csv\n",
            "\n",
            "üìÅ Archivos dentro de 'store-b':\n",
            " Customer - abfss://datalake@adlsstoresproject.dfs.core.windows.net/landing/store-b/customers.csv\n",
            " Sales - abfss://datalake@adlsstoresproject.dfs.core.windows.net/landing/store-b/sales.csv\n",
            "\n",
            "üìÅ Archivos dentro de 'store-c':\n",
            " Customer - abfss://datalake@adlsstoresproject.dfs.core.windows.net/landing/store-c/customers.csv\n",
            " Sales - abfss://datalake@adlsstoresproject.dfs.core.windows.net/landing/store-c/sales.csv\n",
            "\n",
            "‚úÖ Listado completo.\n"
          ]
        }
      ],
      "source": [
        "file_system_client = service_client.get_file_system_client(container_name)\n",
        "\n",
        "# --- Buscar carpetas que empiecen por 'store-' ---\n",
        "store_folders = [\n",
        "    path.name.split(\"/\")[1] for path in file_system_client.get_paths(root_source_folder)\n",
        "    if path.is_directory and path.name.split(\"/\")[1].startswith(\"store-\")\n",
        "]\n",
        "\n",
        "print(f\"üîç Tiendas encontradas: {store_folders}\")\n",
        "\n",
        "csv_sales_paths = []\n",
        "csv_customers_paths = []\n",
        "\n",
        "# --- Recorrer cada carpeta de tienda ---\n",
        "for store in store_folders:\n",
        "    print(f\"\\nüìÅ Archivos dentro de '{store}':\")\n",
        "    store_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{root_source_folder}/{store}\"\n",
        "\n",
        "    try:\n",
        "        # Listar los archivos dentro de landing/store-X\n",
        "        files_in_store = file_system_client.get_paths(f\"{root_source_folder}/{store}\")\n",
        "\n",
        "        csv_files = [f.name.split('/')[-1] for f in files_in_store if not f.is_directory and f.name.endswith(\".csv\")]\n",
        "\n",
        "        if csv_files:\n",
        "            for f in csv_files:\n",
        "                if (f == \"customers.csv\"):\n",
        "                    csv_customers_paths.append(f\"{store_path}/{f}\")\n",
        "                    print(f\" Customer - {store_path}/{f}\")\n",
        "                \n",
        "                if (f == \"sales.csv\"):\n",
        "                    csv_sales_paths.append(f\"{store_path}/{f}\")\n",
        "                    print(f\" Sales - {store_path}/{f}\")\n",
        "                \n",
        "        else:\n",
        "            print(\"  (No se encontraron archivos CSV)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error al listar archivos de '{store}': {str(e)}\")\n",
        "\n",
        "print(\"\\n‚úÖ Listado completo.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Unir todos los archivos **sales.csv** y **customers.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-10-17T17:43:20.9735257Z",
              "execution_start_time": "2025-10-17T17:43:01.986505Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "b72db047-8345-4ad3-9542-6942366aa352",
              "queued_time": "2025-10-17T17:43:01.2200631Z",
              "session_id": "52",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkpoolnew",
              "state": "finished",
              "statement_id": 7,
              "statement_ids": [
                7
              ]
            },
            "text/plain": [
              "StatementMeta(sparkpoolnew, 52, 7, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ Archivos a procesar:\n",
            " - abfss://datalake@adlsstoresproject.dfs.core.windows.net/landing/store-a/sales.csv\n",
            " - abfss://datalake@adlsstoresproject.dfs.core.windows.net/landing/store-b/sales.csv\n",
            " - abfss://datalake@adlsstoresproject.dfs.core.windows.net/landing/store-c/sales.csv\n",
            "‚úÖ Archivos 'sales' unificados correctamente. Total de registros: 47000\n",
            "+-------+----------+-----------+--------+----------+------------+----------+---------+-------+-------------------+----+-----+---+------------+\n",
            "|sale_id|product_id|customer_id|quantity|unit_price|total_amount| sale_date|sale_time|  store|         created_at|year|month|day|is_validated|\n",
            "+-------+----------+-----------+--------+----------+------------+----------+---------+-------+-------------------+----+-----+---+------------+\n",
            "|  98001|        64|         10|       3|  85766.43|   257299.29|2025-10-17| 12:28:28|store-a|2025-10-17 12:43:02|2025|   10| 17|       false|\n",
            "|  98002|        28|         95|       9|  40029.52|   360265.68|2025-10-17| 07:51:54|store-a|2025-10-17 12:43:02|2025|   10| 17|       false|\n",
            "|  98003|        15|        130|      10|  62437.36|    624373.6|2025-10-17| 06:31:07|store-a|2025-10-17 12:43:02|2025|   10| 17|       false|\n",
            "|  98004|        26|         86|       2|  62862.34|   125724.68|2025-10-17| 05:04:32|store-a|2025-10-17 12:43:02|2025|   10| 17|       false|\n",
            "|  98005|         4|        129|       9|  72969.97|   656729.73|2025-10-17| 12:57:51|store-a|2025-10-17 12:43:02|2025|   10| 17|       false|\n",
            "+-------+----------+-----------+--------+----------+------------+----------+---------+-------+-------------------+----+-----+---+------------+\n",
            "only showing top 5 rows\n",
            "\n",
            " - abfss://datalake@adlsstoresproject.dfs.core.windows.net/landing/store-a/customers.csv\n",
            " - abfss://datalake@adlsstoresproject.dfs.core.windows.net/landing/store-b/customers.csv\n",
            " - abfss://datalake@adlsstoresproject.dfs.core.windows.net/landing/store-c/customers.csv\n",
            "‚úÖ Archivos 'customer' unificados correctamente. Total de registros: 95\n",
            "+-----------+---------+--------+--------------------+--------------------+-------+-------------------+----+-----+---+------------+\n",
            "|customer_id|firstname|lastname|               email|               phone|  store|         created_at|year|month|day|is_validated|\n",
            "+-----------+---------+--------+--------------------+--------------------+-------+-------------------+----+-----+---+------------+\n",
            "|        131| &Remigio|    Ad√°n|                null|        347147023770|store-a|2025-10-17 12:43:02|2025|   10| 17|       false|\n",
            "|        132|Marcelino| Chav¬¨es|beatriz35example.net|            54199416|store-a|2025-10-17 12:43:02|2025|   10| 17|       false|\n",
            "|        133|    Bel√©n| Torrent|graciagrande@exam...|354348498643-2369...|store-a|2025-10-17 12:43:02|2025|   10| 17|       false|\n",
            "|        134| Casemiro|   Agudo| notengo@hotmail.com|        727498106598|store-a|2025-10-17 12:43:02|2025|   10| 17|       false|\n",
            "|        135|      Paz|  C&erd√°|baldomerodomingue...|    (572)-881-584256|store-a|2025-10-17 12:43:02|2025|   10| 17|       false|\n",
            "+-----------+---------+--------+--------------------+--------------------+-------+-------------------+----+-----+---+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import current_timestamp\n",
        "from pyspark.sql.functions import lit\n",
        "from datetime import datetime\n",
        "\n",
        "# Zona horaria de Colombia\n",
        "colombia_tz = pytz.timezone(\"America/Bogota\")\n",
        "timestamp_now = datetime.now(colombia_tz).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "today = datetime.now()\n",
        "year = today.strftime(\"%Y\")\n",
        "month = today.strftime(\"%m\")\n",
        "day = today.strftime(\"%d\")\n",
        "\n",
        "# Inicializar DataFrame vac√≠o\n",
        "df_sales = None\n",
        "df_customers = None\n",
        "\n",
        "print(\"üìÇ Archivos a procesar:\")\n",
        "\n",
        "for sale_path in csv_sales_paths:\n",
        "    print(f\" - {sale_path}\")\n",
        "\n",
        "    # ‚úÖ Extraer el nombre de la store desde la ruta (por ejemplo, \"store-a\")\n",
        "    store_name = sale_path.split(\"/\")[-2]  # toma el pen√∫ltimo fragmento de la ruta\n",
        "    \n",
        "    # Leer el CSV\n",
        "    df_temp = spark.read.option(\"header\", \"true\").csv(sale_path)\n",
        "    \n",
        "    # Agregar columnas\n",
        "    df_temp = (\n",
        "        df_temp\n",
        "            .withColumn(\"store\", lit(store_name))\n",
        "            .withColumn(\"created_at\", lit(timestamp_now))\n",
        "            .withColumn(\"year\", lit(year))\n",
        "            .withColumn(\"month\", lit(month))\n",
        "            .withColumn(\"day\", lit(day))\n",
        "            .withColumn(\"is_validated\", lit(False))\n",
        "    )\n",
        "    \n",
        "    # Unir con el DataFrame principal\n",
        "    if df_sales is None:\n",
        "        df_sales = df_temp\n",
        "    else:\n",
        "        df_sales = df_sales.unionByName(df_temp, allowMissingColumns=True)\n",
        "\n",
        "if df_sales is not None:\n",
        "    print(f\"‚úÖ Archivos 'sales' unificados correctamente. Total de registros: {df_sales.count()}\")\n",
        "    df_sales.show(5)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No se encontraron archivos 'customer' para unificar.\")\n",
        "\n",
        "######################\n",
        "\n",
        "for customer_path in csv_customers_paths:\n",
        "    print(f\" - {customer_path}\")\n",
        "\n",
        "    # ‚úÖ Extraer el nombre de la store desde la ruta (por ejemplo, \"store-a\")\n",
        "    store_name = customer_path.split(\"/\")[-2]  # toma el pen√∫ltimo fragmento de la ruta\n",
        "    \n",
        "    # Leer el CSV\n",
        "    df_temp = spark.read.option(\"header\", \"true\").csv(customer_path)\n",
        "    \n",
        "    # Agregar columnas\n",
        "    df_temp = (\n",
        "        df_temp\n",
        "            .withColumn(\"store\", lit(store_name))\n",
        "            .withColumn(\"created_at\", lit(timestamp_now))\n",
        "            .withColumn(\"year\", lit(year))\n",
        "            .withColumn(\"month\", lit(month))\n",
        "            .withColumn(\"day\", lit(day))\n",
        "            .withColumn(\"is_validated\", lit(False))\n",
        "    )\n",
        "    \n",
        "    # Unir con el DataFrame principal\n",
        "    if df_customers is None:\n",
        "        df_customers = df_temp\n",
        "    else:\n",
        "        df_customers = df_customers.unionByName(df_temp, allowMissingColumns=True)\n",
        "\n",
        "if df_customers is not None:\n",
        "    print(f\"‚úÖ Archivos 'customer' unificados correctamente. Total de registros: {df_customers.count()}\")\n",
        "    df_customers.show(5)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No se encontraron archivos 'customer' para unificar.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Guardar **df_sales** como **sales.parquet** y Crear Delta de **sales** particionado por AAAA/MM/DD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-10-17T17:43:21.185945Z",
              "execution_start_time": "2025-10-17T17:43:20.9838197Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "b00727fc-d928-4283-b6e4-bd5db57f395e",
              "queued_time": "2025-10-17T17:43:01.3215616Z",
              "session_id": "52",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkpoolnew",
              "state": "finished",
              "statement_id": 8,
              "statement_ids": [
                8
              ]
            },
            "text/plain": [
              "StatementMeta(sparkpoolnew, 52, 8, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pyspark.sql.functions import concat_ws\n",
        "from delta.tables import DeltaTable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-10-17T17:43:54.9459363Z",
              "execution_start_time": "2025-10-17T17:43:21.2020674Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "72a550dc-b21b-4d35-b9b4-ca711da00561",
              "queued_time": "2025-10-17T17:43:01.5017366Z",
              "session_id": "52",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkpoolnew",
              "state": "finished",
              "statement_id": 9,
              "statement_ids": [
                9
              ]
            },
            "text/plain": [
              "StatementMeta(sparkpoolnew, 52, 9, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Carpeta Parquet guardada en: abfss://datalake@adlsstoresproject.dfs.core.windows.net/temporal_files/sales/\n",
            "‚úÖ Merge realizado en Delta: abfss://datalake@adlsstoresproject.dfs.core.windows.net/processed/sales/\n",
            "‚úÖ Archivo Delta de 'sales' guardado en: abfss://datalake@adlsstoresproject.dfs.core.windows.net/processed/sales/\n"
          ]
        }
      ],
      "source": [
        "if df_sales is not None:\n",
        "    # üìÇ Rutas (parquet en carpeta, no archivo individual)\n",
        "    input_path = \"abfss://datalake@adlsstoresproject.dfs.core.windows.net/temporal_files/sales/\"\n",
        "    output_path = f\"abfss://datalake@adlsstoresproject.dfs.core.windows.net/processed/sales/\"\n",
        "\n",
        "    # üíæ Guardar el DataFrame en formato Parquet en carpeta\n",
        "    df_sales.write.mode(\"overwrite\").parquet(input_path)\n",
        "    print(f\"‚úÖ Carpeta Parquet guardada en: {input_path}\")\n",
        "\n",
        "    # üßæ Leer Parquet desde la carpeta temporal\n",
        "    spark.catalog.clearCache()  # asegurarse de limpiar cache nuevamente\n",
        "    df_sales = spark.read.parquet(input_path)\n",
        "\n",
        "    # üß† Crear columna de llave compuesta\n",
        "    df_sales = df_sales.withColumn(\"unique_id\", concat_ws(\"_\", \"sale_id\", \"store\"))\n",
        "\n",
        "    # üßπ Eliminar duplicados\n",
        "    df_sales = df_sales.dropDuplicates([\"unique_id\"])\n",
        "\n",
        "    # üíæ Crear carpetas si no existen\n",
        "    folders = [\"processed\", \"sales\"]\n",
        "    path = \"\"\n",
        "    for f in folders:\n",
        "        path = f\"{path}/{f}\" if path else f\n",
        "        try:\n",
        "            fs_client.get_directory_client(path).create_directory()\n",
        "        except:\n",
        "            pass  # si ya existe, ignora \n",
        "\n",
        "    # üîÅ MERGE en Delta para evitar duplicados\n",
        "    if DeltaTable.isDeltaTable(spark, output_path):\n",
        "        delta_table = DeltaTable.forPath(spark, output_path)\n",
        "\n",
        "        delta_table.alias(\"target\").merge(\n",
        "            df_sales.alias(\"source\"),\n",
        "            \"target.unique_id = source.unique_id\"\n",
        "        ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
        "\n",
        "        print(f\"‚úÖ Merge realizado en Delta: {output_path}\")\n",
        "    else:\n",
        "        # Si no existe la tabla Delta, se crea por primera vez\n",
        "        df_sales.write.format(\"delta\") \\\n",
        "            .mode(\"overwrite\") \\\n",
        "            .partitionBy(\"year\", \"month\", \"day\") \\\n",
        "            .save(output_path)\n",
        "\n",
        "        print(f\"‚úÖ Tabla Delta creada en: {output_path}\")\n",
        "\n",
        "    print(f\"‚úÖ Archivo Delta de 'sales' guardado en: {output_path}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No se encontraron parquet 'sales' para convertir a Delta.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Guardar **df_customers** como **customers.parquet** y Crear Delta de **customers** particionado por AAAA/MM/DD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-10-17T17:44:07.5282877Z",
              "execution_start_time": "2025-10-17T17:43:54.9572668Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "8e9971c0-1481-47a2-9324-298cef9dfb30",
              "queued_time": "2025-10-17T17:43:01.7470354Z",
              "session_id": "52",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkpoolnew",
              "state": "finished",
              "statement_id": 10,
              "statement_ids": [
                10
              ]
            },
            "text/plain": [
              "StatementMeta(sparkpoolnew, 52, 10, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Carpeta Parquet guardada en: abfss://datalake@adlsstoresproject.dfs.core.windows.net/temporal_files/customers/\n",
            "‚úÖ Merge realizado en Delta: abfss://datalake@adlsstoresproject.dfs.core.windows.net/processed/customers/\n",
            "‚úÖ Archivo Delta de 'customers' guardado en: abfss://datalake@adlsstoresproject.dfs.core.windows.net/processed/customers/\n"
          ]
        }
      ],
      "source": [
        "if df_customers is not None:\n",
        "    # üìÇ Rutas (parquet en carpeta, no archivo individual)\n",
        "    input_path = \"abfss://datalake@adlsstoresproject.dfs.core.windows.net/temporal_files/customers/\"\n",
        "    output_path = f\"abfss://datalake@adlsstoresproject.dfs.core.windows.net/processed/customers/\"\n",
        "\n",
        "    # üíæ Guardar el DataFrame en formato Parquet en carpeta\n",
        "    df_customers.write.mode(\"overwrite\").parquet(input_path)\n",
        "    print(f\"‚úÖ Carpeta Parquet guardada en: {input_path}\")\n",
        "\n",
        "    # üßæ Leer Parquet desde la carpeta temporal\n",
        "    spark.catalog.clearCache()  # asegurarse de limpiar cache nuevamente\n",
        "    df_customers = spark.read.parquet(input_path)\n",
        "\n",
        "    # üß† Crear columna de llave compuesta\n",
        "    df_customers = df_customers.withColumn(\"unique_id\", concat_ws(\"_\", \"customer_id\", \"store\"))\n",
        "\n",
        "    # üßπ Eliminar duplicados\n",
        "    df_customers = df_customers.dropDuplicates([\"unique_id\"])\n",
        "\n",
        "    # üíæ Crear carpetas si no existen\n",
        "    folders = [\"processed\", \"customers\"]\n",
        "    path = \"\"\n",
        "    for f in folders:\n",
        "        path = f\"{path}/{f}\" if path else f\n",
        "        try:\n",
        "            fs_client.get_directory_client(path).create_directory()\n",
        "        except:\n",
        "            pass  # si ya existe, ignora \n",
        "\n",
        "    # üîÅ MERGE en Delta para evitar duplicados\n",
        "    if DeltaTable.isDeltaTable(spark, output_path):\n",
        "        delta_table = DeltaTable.forPath(spark, output_path)\n",
        "\n",
        "        delta_table.alias(\"target\").merge(\n",
        "            df_customers.alias(\"source\"),\n",
        "            \"target.unique_id = source.unique_id\"\n",
        "        ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
        "\n",
        "        print(f\"‚úÖ Merge realizado en Delta: {output_path}\")\n",
        "    else:\n",
        "        # Si no existe la tabla Delta, se crea por primera vez\n",
        "        df_customers.write.format(\"delta\") \\\n",
        "            .mode(\"overwrite\") \\\n",
        "            .partitionBy(\"year\", \"month\", \"day\") \\\n",
        "            .save(output_path)\n",
        "\n",
        "        print(f\"‚úÖ Tabla Delta creada en: {output_path}\")\n",
        "\n",
        "    print(f\"‚úÖ Archivo Delta de 'customers' guardado en: {output_path}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No se encontraron parquet 'customers' para convertir a Delta.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Identificar archivos **products.csv** y **suppliers.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-10-17T17:44:09.4050172Z",
              "execution_start_time": "2025-10-17T17:44:07.5412617Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "f8eebce1-2138-4800-8508-9110259cf3d6",
              "queued_time": "2025-10-17T17:43:01.8708873Z",
              "session_id": "52",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkpoolnew",
              "state": "finished",
              "statement_id": 11,
              "statement_ids": [
                11
              ]
            },
            "text/plain": [
              "StatementMeta(sparkpoolnew, 52, 11, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÑ Archivo encontrado: abfss://datalake@adlsstoresproject.dfs.core.windows.net/landing/products/products.csv\n",
            "+----------+--------------------+-------------+---------+-----------+-------------------+----+-----+---+\n",
            "|product_id|        product_name|product_price|available|supplier_id|         created_at|year|month|day|\n",
            "+----------+--------------------+-------------+---------+-----------+-------------------+----+-----+---+\n",
            "|         1|Filete de Tilapia...|      30319.3|     True|         38|2025-10-17 12:43:02|2025|   10| 17|\n",
            "|         2|Mantequilla Alpin...|     58827.81|    False|         25|2025-10-17 12:43:02|2025|   10| 17|\n",
            "|         3|            Yuca 1kg|     57341.62|    False|          3|2025-10-17 12:43:02|2025|   10| 17|\n",
            "|         4|Tomate de √Årbol 500g|     15978.83|    False|         36|2025-10-17 12:43:02|2025|   10| 17|\n",
            "|         5|       Jet Chocolate|     30762.35|    False|         15|2025-10-17 12:43:02|2025|   10| 17|\n",
            "+----------+--------------------+-------------+---------+-----------+-------------------+----+-----+---+\n",
            "only showing top 5 rows\n",
            "\n",
            "üìÑ Archivo encontrado: abfss://datalake@adlsstoresproject.dfs.core.windows.net/landing/suppliers/suppliers.csv\n",
            "+-----------+--------------------+--------------------+--------------------+-------------------+----+-----+---+\n",
            "|supplier_id|       supplier_name|       contact_email|               phone|         created_at|year|month|day|\n",
            "+-----------+--------------------+--------------------+--------------------+-------------------+----+-----+---+\n",
            "|          1|Comercial Tamayo ...|manolacontreras@c...|        198371087458|2025-10-17 12:43:02|2025|   10| 17|\n",
            "|          2|Banca Privada del...| notengo@hotmail.com|126754285322-4902...|2025-10-17 12:43:02|2025|   10| 17|\n",
            "|          3|Compa√±√≠a Arrieta ...|                null|        926155859497|2025-10-17 12:43:02|2025|   10| 17|\n",
            "|          4|Tirado & Asociado...|                null|539323491601-7831...|2025-10-17 12:43:02|2025|   10| 17|\n",
            "|          5|Restauraci√≥n RU S...|mesaartemio@valer...|        142158659618|2025-10-17 12:43:02|2025|   10| 17|\n",
            "+-----------+--------------------+--------------------+--------------------+-------------------+----+-----+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --- Rutas relativas ---\n",
        "products_folder_path = \"landing/products\"\n",
        "products_file_name = \"products.csv\"\n",
        "products_file_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{products_folder_path}/{products_file_name}\"\n",
        "\n",
        "suppliers_folder_path = \"landing/suppliers\"\n",
        "suppliers_file_name = \"suppliers.csv\"\n",
        "suppliers_file_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{suppliers_folder_path}/{suppliers_file_name}\"\n",
        "\n",
        "# --- Funci√≥n corregida ---\n",
        "def read_and_enrich_csv(folder_path, file_name, full_path):\n",
        "    try:\n",
        "        files_in_folder = file_system_client.get_paths(path=folder_path)\n",
        "        found_files = [f.name.split('/')[-1] for f in files_in_folder if not f.is_directory]\n",
        "\n",
        "        if file_name in found_files:\n",
        "            print(f\"üìÑ Archivo encontrado: {full_path}\")\n",
        "            df = spark.read.option(\"header\", True).csv(full_path)\n",
        "\n",
        "            df = (\n",
        "                df.withColumn(\"created_at\", lit(timestamp_now))\n",
        "                  .withColumn(\"year\", lit(year))\n",
        "                  .withColumn(\"month\", lit(month))\n",
        "                  .withColumn(\"day\", lit(day))\n",
        "            )\n",
        "\n",
        "            df.show(5)\n",
        "            return df\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è El archivo '{file_name}' no se encontr√≥ en '{folder_path}'.\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error al acceder a '{folder_path}': {str(e)}\")\n",
        "        return None\n",
        "\n",
        "df_products = read_and_enrich_csv(products_folder_path, products_file_name, products_file_path)\n",
        "df_suppliers = read_and_enrich_csv(suppliers_folder_path, suppliers_file_name, suppliers_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Guardar **df** de products y suppliers como **Parquet** y generar archivos **Delta**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-10-17T17:44:22.0010656Z",
              "execution_start_time": "2025-10-17T17:44:09.4176013Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "44c71b2b-f312-45cd-a1ec-77556f1191a7",
              "queued_time": "2025-10-17T17:43:01.930615Z",
              "session_id": "52",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkpoolnew",
              "state": "finished",
              "statement_id": 12,
              "statement_ids": [
                12
              ]
            },
            "text/plain": [
              "StatementMeta(sparkpoolnew, 52, 12, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Carpeta Parquet guardada en: abfss://datalake@adlsstoresproject.dfs.core.windows.net/temporal_files/products/\n",
            "‚úÖ Tabla Delta de 'products' creada en: abfss://datalake@adlsstoresproject.dfs.core.windows.net/processed/products/\n",
            "‚úÖ Carpeta Parquet guardada en: abfss://datalake@adlsstoresproject.dfs.core.windows.net/temporal_files/suppliers/\n",
            "‚úÖ Tabla Delta de 'suppliers' creada en: abfss://datalake@adlsstoresproject.dfs.core.windows.net/processed/suppliers/\n"
          ]
        }
      ],
      "source": [
        "# --- Guardar df_products como Delta ---\n",
        "if df_products is not None:\n",
        "    products_parquet_path = \"abfss://datalake@adlsstoresproject.dfs.core.windows.net/temporal_files/products/\"\n",
        "    products_delta_path = \"abfss://datalake@adlsstoresproject.dfs.core.windows.net/processed/products/\"\n",
        "\n",
        "    # üíæ Guardar en Parquet temporal\n",
        "    df_products.write.mode(\"overwrite\").parquet(products_parquet_path)\n",
        "    print(f\"‚úÖ Carpeta Parquet guardada en: {products_parquet_path}\")\n",
        "\n",
        "    # üßæ Leer Parquet desde carpeta temporal\n",
        "    spark.catalog.clearCache()\n",
        "    df_products = spark.read.parquet(products_parquet_path)\n",
        "\n",
        "    # üíæ Crear carpetas si no existen\n",
        "    folders = [\"processed\", \"products\"]\n",
        "    path = \"\"\n",
        "    for f in folders:\n",
        "        path = f\"{path}/{f}\" if path else f\n",
        "        try:\n",
        "            fs_client.get_directory_client(path).create_directory()\n",
        "        except:\n",
        "            pass  # si ya existe, ignora\n",
        "\n",
        "    # üß† Carga completa en Delta (sin merge)\n",
        "    df_products.write.format(\"delta\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .save(products_delta_path)\n",
        "\n",
        "    print(f\"‚úÖ Tabla Delta de 'products' creada en: {products_delta_path}\")\n",
        "\n",
        "# --- Guardar df_suppliers como Delta ---\n",
        "if df_suppliers is not None:\n",
        "    suppliers_parquet_path = \"abfss://datalake@adlsstoresproject.dfs.core.windows.net/temporal_files/suppliers/\"\n",
        "    suppliers_delta_path = \"abfss://datalake@adlsstoresproject.dfs.core.windows.net/processed/suppliers/\"\n",
        "\n",
        "    # üíæ Guardar en Parquet temporal\n",
        "    df_suppliers.write.mode(\"overwrite\").parquet(suppliers_parquet_path)\n",
        "    print(f\"‚úÖ Carpeta Parquet guardada en: {suppliers_parquet_path}\")\n",
        "\n",
        "    # üßæ Leer Parquet desde carpeta temporal\n",
        "    spark.catalog.clearCache()\n",
        "    df_suppliers = spark.read.parquet(suppliers_parquet_path)\n",
        "\n",
        "    # üíæ Crear carpetas si no existen\n",
        "    folders = [\"processed\", \"suppliers\"]\n",
        "    path = \"\"\n",
        "    for f in folders:\n",
        "        path = f\"{path}/{f}\" if path else f\n",
        "        try:\n",
        "            fs_client.get_directory_client(path).create_directory()\n",
        "        except:\n",
        "            pass  # si ya existe, ignora\n",
        "\n",
        "    # üß† Carga completa en Delta (sin merge)\n",
        "    df_suppliers.write.format(\"delta\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .save(suppliers_delta_path)\n",
        "\n",
        "    print(f\"‚úÖ Tabla Delta de 'suppliers' creada en: {suppliers_delta_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Create Lake Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-10-17T17:44:23.1412166Z",
              "execution_start_time": "2025-10-17T17:44:22.0113999Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "b8919212-9f90-4809-bafa-31d807c97182",
              "queued_time": "2025-10-17T17:43:01.984707Z",
              "session_id": "52",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkpoolnew",
              "state": "finished",
              "statement_id": 13,
              "statement_ids": [
                13
              ]
            },
            "text/plain": [
              "StatementMeta(sparkpoolnew, 52, 13, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Lake Database 'bronze_db' creada (o ya existente)\n",
            "üìÅ Ubicaci√≥n: abfss://datalake@adlsstoresproject.dfs.core.windows.net/processed\n",
            "+--------------+-----------------------------------------------------------------+\n",
            "|info_name     |info_value                                                       |\n",
            "+--------------+-----------------------------------------------------------------+\n",
            "|Catalog Name  |spark_catalog                                                    |\n",
            "|Namespace Name|bronze_db                                                        |\n",
            "|Comment       |                                                                 |\n",
            "|Location      |abfss://datalake@adlsstoresproject.dfs.core.windows.net/bronze_db|\n",
            "|Owner         |                                                                 |\n",
            "|Properties    |((IsSyMSCDMDatabase,true))                                       |\n",
            "+--------------+-----------------------------------------------------------------+\n",
            "\n",
            "+---------+\n",
            "|namespace|\n",
            "+---------+\n",
            "|  default|\n",
            "|bronze_db|\n",
            "|silver_db|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Nombre de la base de datos\n",
        "database_name = \"bronze_db\"\n",
        "\n",
        "# Ruta donde vivir√° la base (carpeta processed)\n",
        "database_location = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{root_processed_folder}\"\n",
        "\n",
        "# Crear la Lake Database Delta si no existe\n",
        "spark.sql(f\"\"\"\n",
        "CREATE DATABASE IF NOT EXISTS {database_name}\n",
        "LOCATION '{database_location}'\n",
        "\"\"\")\n",
        "\n",
        "print(f\"‚úÖ Lake Database '{database_name}' creada (o ya existente)\")\n",
        "print(f\"üìÅ Ubicaci√≥n: {database_location}\")\n",
        "\n",
        "spark.sql(\"DESCRIBE DATABASE EXTENDED bronze_db\").show(truncate=False)\n",
        "spark.sql(\"SHOW DATABASES\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Create Delta tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-10-17T17:44:23.3527528Z",
              "execution_start_time": "2025-10-17T17:44:23.151747Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "1998bc4a-c6a9-4f2c-b7e0-7c7f27d8fd7c",
              "queued_time": "2025-10-17T17:43:02.0597687Z",
              "session_id": "52",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkpoolnew",
              "state": "finished",
              "statement_id": 14,
              "statement_ids": [
                14
              ]
            },
            "text/plain": [
              "StatementMeta(sparkpoolnew, 52, 14, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Eliminar tablas:\n",
        "#spark.sql(\"DROP TABLE IF EXISTS bronze_db.customers\")\n",
        "#spark.sql(\"DROP TABLE IF EXISTS bronze_db.sales\")\n",
        "#spark.sql(\"DROP TABLE IF EXISTS bronze_db.products\")\n",
        "#spark.sql(\"DROP TABLE IF EXISTS bronze_db.suppliers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Table **bronze_db.sales**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-10-17T17:44:27.3066241Z",
              "execution_start_time": "2025-10-17T17:44:23.3647109Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "ea43f396-8dae-4207-bf18-11850ff60ce3",
              "queued_time": "2025-10-17T17:43:02.2704947Z",
              "session_id": "52",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkpoolnew",
              "state": "finished",
              "statement_id": 15,
              "statement_ids": [
                15
              ]
            },
            "text/plain": [
              "StatementMeta(sparkpoolnew, 52, 15, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Tabla 'bronze_db.sales' registrada correctamente en el Lake Database\n"
          ]
        }
      ],
      "source": [
        "# Crear la tabla dentro del Lake Database bronze_db\n",
        "spark.sql(f\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS bronze_db.sales\n",
        "    USING DELTA\n",
        "    LOCATION 'abfss://datalake@adlsstoresproject.dfs.core.windows.net/processed/sales'\n",
        "\"\"\")\n",
        "\n",
        "print(\"‚úÖ Tabla 'bronze_db.sales' registrada correctamente en el Lake Database\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Table **bronze_db.customers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-10-17T17:44:30.1012001Z",
              "execution_start_time": "2025-10-17T17:44:27.3179271Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "6b99a667-afed-4f05-a257-8c1f89bc8c3c",
              "queued_time": "2025-10-17T17:43:02.3663271Z",
              "session_id": "52",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkpoolnew",
              "state": "finished",
              "statement_id": 16,
              "statement_ids": [
                16
              ]
            },
            "text/plain": [
              "StatementMeta(sparkpoolnew, 52, 16, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Tabla 'bronze_db.customers' registrada correctamente en el Lake Database\n"
          ]
        }
      ],
      "source": [
        "# Crear la tabla dentro del Lake Database bronze_db\n",
        "spark.sql(f\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS bronze_db.customers\n",
        "    USING DELTA\n",
        "    LOCATION 'abfss://datalake@adlsstoresproject.dfs.core.windows.net/processed/customers'\n",
        "\"\"\")\n",
        "\n",
        "print(\"‚úÖ Tabla 'bronze_db.customers' registrada correctamente en el Lake Database\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Table **bronze_db.products**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-10-17T17:44:30.6796555Z",
              "execution_start_time": "2025-10-17T17:44:30.1126555Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "e689d247-2c13-4d27-93fc-ded17da6ee56",
              "queued_time": "2025-10-17T17:43:02.4405178Z",
              "session_id": "52",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkpoolnew",
              "state": "finished",
              "statement_id": 17,
              "statement_ids": [
                17
              ]
            },
            "text/plain": [
              "StatementMeta(sparkpoolnew, 52, 17, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Tabla 'bronze_db.products' registrada correctamente en el Lake Database\n"
          ]
        }
      ],
      "source": [
        "# Crear la tabla dentro del Lake Database bronze_db\n",
        "spark.sql(f\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS bronze_db.products\n",
        "    USING DELTA\n",
        "    LOCATION 'abfss://datalake@adlsstoresproject.dfs.core.windows.net/processed/products'\n",
        "\"\"\")\n",
        "\n",
        "print(\"‚úÖ Tabla 'bronze_db.products' registrada correctamente en el Lake Database\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Table **bronze_db.suppliers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-10-17T17:44:31.275764Z",
              "execution_start_time": "2025-10-17T17:44:30.6909568Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "07fa2741-fd4d-44cd-9640-d20c8c34c315",
              "queued_time": "2025-10-17T17:43:02.5049171Z",
              "session_id": "52",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkpoolnew",
              "state": "finished",
              "statement_id": 18,
              "statement_ids": [
                18
              ]
            },
            "text/plain": [
              "StatementMeta(sparkpoolnew, 52, 18, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Tabla 'bronze_db.suppliers' registrada correctamente en el Lake Database\n"
          ]
        }
      ],
      "source": [
        "# Crear la tabla dentro del Lake Database bronze_db\n",
        "spark.sql(f\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS bronze_db.suppliers\n",
        "    USING DELTA\n",
        "    LOCATION 'abfss://datalake@adlsstoresproject.dfs.core.windows.net/processed/suppliers'\n",
        "\"\"\")\n",
        "\n",
        "print(\"‚úÖ Tabla 'bronze_db.suppliers' registrada correctamente en el Lake Database\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
