# Pipeline de Ingesta y Transformaci√≥n de Datos en Azure para Ventas de Retail


## Descripci√≥n

Este proyecto demuestra un pipeline completo de ingenier√≠a de datos en Microsoft Azure. 
Automatiza la ingesta, transformaci√≥n y almacenamiento de datos de ventas de retail usando 
Azure Blob Storage, ADLS Gen2, Azure Data Factory y Synapse Analytics. 
Los datasets finales se estructuran en capas Bronze, Silver y Gold.


## Tabla de Contenidos

- [Arquitectura](#arquitectura)
- [Tecnolog√≠as](#tecnolog√≠as)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [Gu√≠a de Configuraci√≥n](#gu√≠a-de-configuraci√≥n)
- [Uso del Proyecto](#uso-del-proyecto)
- [Contribuciones](#contribuciones)
- [Licencia](#licencia)


## Arquitectura

El pipeline sigue una arquitectura tipo Lakehouse y se ejecuta de forma completamente automatizada:

1. **Ingesta programada con Azure Data Factory (ADF)**:
   - El flujo est√° programado para correr diariamente en la madrugada.
   - Toma los archivos CSV de sales y customers generados por cada tienda en Azure Blob Storage.
   - Detecta autom√°ticamente los archivos correspondientes al d√≠a y los copia hacia ADLS Gen2.

2. **Estructura del contenedor Blob Storage**:
 - A continuaci√≥n se muestra la estructura del Blob Storage:
```
   blobstoresproject/
   ‚îÇ
   ‚îî‚îÄ‚îÄ incoming/                      # Carpeta principal de entrada (fuente original de datos)
      ‚îú‚îÄ‚îÄ store-a/
      ‚îÇ   ‚îú‚îÄ‚îÄ sales_20251017.csv      # Archivo de ventas diario de la tienda A
      ‚îÇ   ‚îî‚îÄ‚îÄ customers_20251017.csv  # Archivo de clientes diario de la tienda A
      ‚îÇ
      ‚îú‚îÄ‚îÄ store-b/
      ‚îÇ   ‚îú‚îÄ‚îÄ sales_20251017.csv      # Archivo de ventas diario de la tienda B
      ‚îÇ   ‚îî‚îÄ‚îÄ customers_20251017.csv  # Archivo de clientes diario de la tienda B
      ‚îÇ
      ‚îú‚îÄ‚îÄ store-c/
      ‚îÇ   ‚îú‚îÄ‚îÄ sales_20251017.csv      # Archivo de ventas diario de la tienda C
      ‚îÇ   ‚îî‚îÄ‚îÄ customers_20251017.csv  # Archivo de clientes diario de la tienda C
      ‚îÇ
      ‚îú‚îÄ‚îÄ suppliers/
      ‚îÇ   ‚îî‚îÄ‚îÄ suppliers_20251017.csv  # Datos maestros de proveedores (actualizaci√≥n diaria)
      ‚îÇ
      ‚îî‚îÄ‚îÄ products/
         ‚îî‚îÄ‚îÄ products_20251017.csv    # Datos maestros de productos (actualizaci√≥n diaria)
```
3. **Estructura del contenedor ADLS Gen2**:
 - A continuaci√≥n se muestra la estructura del contenedor ADLS Gen2:
 ```
   adlsstoresproject/
   ‚îÇ
   ‚îú‚îÄ‚îÄ landing/                    # Capa Raw: archivos originales copiados desde Blob Storage por ADF
   ‚îÇ   ‚îú‚îÄ‚îÄ store-a/
   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ customers.csv       # Datos crudos de clientes (d√≠a actual)
   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sales.csv           # Datos crudos de ventas (d√≠a actual)
   ‚îÇ   ‚îÇ
   ‚îÇ   ‚îú‚îÄ‚îÄ store-b/                # Archivos de la tienda B
   ‚îÇ   ‚îî‚îÄ‚îÄ store-c/                # Archivos de la tienda C
   ‚îÇ
   ‚îú‚îÄ‚îÄ processed/                  # Lake Database con tablas Delta (capa Bronze)
   ‚îÇ   ‚îú‚îÄ‚îÄ customers/              # Datos crudos unificados en formato Delta
   ‚îÇ   ‚îú‚îÄ‚îÄ sales/                  # Ventas integradas en formato Delta
   ‚îÇ   ‚îú‚îÄ‚îÄ suppliers/              # Proveedores validados (no pasan a Silver)
   ‚îÇ   ‚îî‚îÄ‚îÄ products/               # Productos validados (no pasan a Silver)
   ‚îÇ
   ‚îú‚îÄ‚îÄ silver/                     # Lake Database con tablas Delta (capa Silver)
   ‚îÇ   ‚îú‚îÄ‚îÄ customers/              # Datos validados y estandarizados de clientes
   ‚îÇ   ‚îî‚îÄ‚îÄ sales/                  # Datos limpios y transformados de ventas
   ‚îÇ
   ‚îú‚îÄ‚îÄ temporal_files/             # Archivos Parquet intermedios generados por notebooks
   ‚îÇ                               # antes de ser convertidos a Delta en la capa Bronze
   ‚îÇ
   ‚îú‚îÄ‚îÄ gold/                       # (Opcional) Estructura referencial, no usada directamente
   ‚îÇ                               # ya que la capa Gold se almacena en SQL Database (Synapse)
   ‚îÇ
   ‚îî‚îÄ‚îÄ synapse/                    # Carpeta creada autom√°ticamente por Synapse Analytics
                                   # usada internamente para la integraci√≥n con notebooks
```
4. **Capa Bronze**:  
   - Esta capa permite unificar la estructura de los datos sin perder los registros originales.   
   - Se crea un **Common Data Model (CDM)** (y se agregan campos: store_id, created_at, year, month, day, unique_id) para unificar todas las ventas y clientes de las N tiendas dentro de sus respectivos datasets en formato Parquet.  
   - Los datos unificados se ingieren en tablas Delta usando notebooks de PySpark. 
   - Posteriormente, se construye la capa Bronze con estos dos modelos (sales y customers) en un **Lake Database** con formato Delta, particionado por A√±o, Mes y D√≠a, con el fin de optimizar consultas futuras.
   - Los datasets **sales** y **customers** se procesan de forma **incremental**, agregando √∫nicamente los nuevos registros de cada d√≠a, lo que permite mantener el hist√≥rico sin reprocesar toda la tabla.  
   - En cambio, los datasets **suppliers** y **products** se cargan mediante un proceso de **carga completa (full load)**, ya que provienen de fuentes maestras estables y se reemplazan en cada ejecuci√≥n.
   - Para manejar la ingesta incremental y poder identificar qu√© registros son procesados en la capa Silver, se agrega el campo "is_validated" a cada tabla incremental.

5. **Capa Silver**:  
   - En esta capa se realiza la limpieza, estandarizaci√≥n y validaci√≥n de los datos provenientes de Bronze.  
   - Se corrigen tipos de datos, formatos de fechas, valores nulos y registros duplicados.  
   - Se generan llaves √∫nicas para garantizar la integridad referencial.  
   - Se aplican **validaciones de calidad de datos** y se ejecutan merges basados en campos clave como `unique_id`.  
   - Los datos resultantes se guardan nuevamente en Delta Tables, listos para su uso anal√≠tico.

6. **Capa Gold**:  
   - En esta capa se preparan datasets agregados y enriquecidos para an√°lisis y consumo en herramientas de BI.  
   - Se crean tablas con m√©tricas resumidas, como ventas totales por tienda, por producto o por cliente.  
   - Los datos procesados se almacenan en una **base de datos SQL (SQL Database)** dentro de **Azure Synapse Analytics**, en lugar de un Lake Database.  
   - Esta decisi√≥n permite una mejor optimizaci√≥n de consultas, uso de √≠ndices y conectividad directa con herramientas de visualizaci√≥n como Power BI.  
   - Los datos se organizan por temas de negocio (ventas, clientes, desempe√±o de tiendas, etc.) y se exponen como una capa de presentaci√≥n confiable para reportes y an√°lisis avanzados.

7. **Orquestaci√≥n y automatizaci√≥n**:  
   - Todo el flujo de datos est√° coordinado mediante **Azure Data Factory (ADF)**, que act√∫a como el orquestador central del proyecto.  
   - El dise√±o est√° basado en una arquitectura **modular y parametrizada**, compuesta por varios *pipelines* independientes que se comunican entre s√≠ mediante el paso de par√°metros din√°micos.  
   - El **pipeline principal (`pl_main`)** no maneja par√°metros, pero controla la ejecuci√≥n completa del proceso:  
     - Primero ejecuta el pipeline de ingesta (`pl_ingest_blob_to_landing`), encargado de copiar los archivos diarios desde **Blob Storage** hacia **ADLS Gen2**.  
     - Luego ejecuta tres *notebooks* de **Azure Synapse Analytics**, responsables de las transformaciones en las capas **Bronze**, **Silver** y **Gold** (`01_move_landing_to_processed`, `02_transform_silver`, `03_model_gold`).  
   - El **pipeline `pl_ingest_blob_to_landing`** identifica los archivos v√°lidos del d√≠a en Blob Storage, filtr√°ndolos por tienda y tipo de dataset (sales, customers, products, suppliers).  
     - Utiliza una variable interna (`currentStore`) para iterar din√°micamente sobre cada tienda.  
     - Por cada tienda o dataset, invoca el pipeline **`pl_get_blob_files`** y le env√≠a par√°metros din√°micos como:  
       - `storesFiles`: lista de archivos filtrados a procesar.  
       - `storeName`: nombre de la tienda o dataset (por ejemplo: `store-a`, `store-b`, `store-c`).  
   - El **pipeline `pl_get_blob_files`** recibe estos par√°metros, itera sobre la lista de archivos y copia cada uno desde **Blob Storage** hacia su carpeta correspondiente en **ADLS Gen2** (`landing/{store}/`).  
     - Los par√°metros se usan dentro de expresiones din√°micas para construir las rutas de origen y destino, lo que permite una ejecuci√≥n completamente automatizada.  
   - Gracias a este dise√±o modular y parametrizado, el flujo puede **procesar m√∫ltiples tiendas de forma escalable**, reutilizando los mismos pipelines sin duplicar c√≥digo ni configuraciones adicionales.


## Tecnolog√≠as

- **Microsoft Azure**
  - **Azure Blob Storage**: Almacenamiento de archivos de entrada (capa *incoming*).  
  - **Azure Data Lake Storage Gen2 (ADLS)**: Almacenamiento principal de datos en capas *landing* y *processed*.  
  - **Azure Data Factory (ADF)**: Orquestaci√≥n de pipelines y ejecuci√≥n de notebooks de Synapse.  
  - **Azure Synapse Analytics**: Procesamiento, transformaci√≥n y modelado de datos.  
    - **Lake Database**: Estructuraci√≥n de datos en capas *Bronze* y *Silver*.  
    - **SQL Database**: Modelado final y an√°lisis en la capa *Gold*.

- **Python / PySpark**: Ingesta, validaci√≥n y transformaci√≥n de datos.  
- **Delta Lake**: Almacenamiento transaccional con versionamiento de datasets.  
- **SQL**: Creaci√≥n de tablas, vistas y consultas anal√≠ticas.  
- **Git & GitHub**: Control de versiones y documentaci√≥n del proyecto.


## Estructura del proyecto
```
üì¶ azure-data-project/
‚îÇ
‚îú‚îÄ‚îÄ üìÅ adf/
‚îÇ   ‚îú‚îÄ‚îÄ pipeline_main.json                        # Pipeline principal orquestador en Azure Data Factory
‚îÇ   ‚îú‚îÄ‚îÄ pipeline_sales.json                       # Pipeline para ingesta de ventas
‚îÇ   ‚îî‚îÄ‚îÄ pipeline_customers.json                   # Pipeline para ingesta de clientes
‚îÇ
‚îú‚îÄ‚îÄ üìÅ data/
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ incoming/                              # Archivos originales (simulaci√≥n del Blob Storage)
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ landing/                               # Datos copiados a la capa landing de un d√≠a en particular (ADLS)
‚îÇ
‚îú‚îÄ‚îÄ üìÅ docs/
‚îÇ   ‚îú‚îÄ‚îÄ project architecture.png                  # Diagrama de arquitectura del proyecto
‚îÇ   ‚îú‚îÄ‚îÄ adf_pipeline_get_blob_files.png           # Pipeline para identificar archivos en el Blob Storage
‚îÇ   ‚îú‚îÄ‚îÄ adf_pipeline_ingest_blob_to_landing.png   # Pipeline para mover archivos desde Blos Storage hacia ADSL Gen2
‚îÇ   ‚îî‚îÄ‚îÄ adf_pipeline_main.png                     # Pipeline principal
‚îÇ
‚îú‚îÄ‚îÄ üìÅ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_move_landing_to_processed.ipynb        # Ingesta y creaci√≥n de tablas Delta en Bronze
‚îÇ   ‚îú‚îÄ‚îÄ 02_transform_silver.ipynb                 # Limpieza y estandarizaci√≥n en Silver
‚îÇ   ‚îî‚îÄ‚îÄ 03_model_gold.ipynb                       # Modelado final y carga en SQL Database
‚îÇ
‚îú‚îÄ‚îÄ üìÅ sql/
‚îÇ   ‚îú‚îÄ‚îÄ number_of_sales_by_store_and_year.sql     # Cantidad de ventas por tienda por a√±o
‚îÇ   ‚îú‚îÄ‚îÄ top_15_best_selling_products.sql          # Top 15 de productos m√°s vendidos
‚îÇ   ‚îî‚îÄ‚îÄ total_sales_by_store_and_year.sql         # Total dinero vendido por tienda por a√±o
‚îÇ
‚îî‚îÄ‚îÄ README-ES.md                                  # Documentaci√≥n principal del proyecto
```

## Gu√≠a de configuraci√≥n

Esta gu√≠a describe los pasos generales y configuraciones necesarias para implementar la soluci√≥n en un entorno de **Microsoft Azure**.

1. **Crear los recursos base en Azure**  
   - **Resource Group:** `rgoup-stores`  
   - **Regi√≥n:** East US 2  
   - **Servicios principales:**  
     - Azure Data Lake Storage Gen2 (`adlsstoresproject`)  
     - Azure Blob Storage (`blobstoresproject`)  
     - Azure Synapse Analytics (`synapse-stores`)
     - Azure SQL Database (`stores-sql-db-gold`)    
     - Azure Data Factory (`adf-storesproject`)  

2. **Estructurar los contenedores**  
   - En **Blob Storage**, crear la carpeta `incoming/` con subcarpetas por tienda (`store-a`, `store-b`, `store-c`) y carpetas globales (`products/`, `suppliers/`).  
   - En **ADLS Gen2**, crear las carpetas base:  
     `landing/`, `processed/`, `silver/`, `temporal_files/` y opcionalmente `gold/`.

3. **Configurar los pipelines en Azure Data Factory (ADF)**  
   - Ver importaciones de los pipelines en 'azure-data-pipelines-project/adf/'.
   - Implementar pipelines:
     - `pl_get_blob_files.json`  
     - `pl_ingest_blob_to_landing.json`  
     - `pl_main.json`
   - Configurar las conexiones (`Linked Services`) a:
     - Blob Storage (fuente)
     - ADLS Gen2 (destino)
     - Synapse (para notebooks)
   - Asegurar que los par√°metros `storesFiles` y `storeName` est√©n correctamente enlazados entre los pipelines.

4. **Configurar notebooks de transformaci√≥n en Synapse Analytics**  
   - Crear los notebooks:
     - `01_move_landing_to_processed` ‚Üí genera la capa Bronze.  
     - `02_transform_silver` ‚Üí limpia y valida los datos para la capa Silver.  
     - `03_model_gold` ‚Üí carga los datos transformados hacia SQL Database (capa Gold).  
   - Vincular los notebooks con el pipeline principal `pl_main` en ADF.

5. **Configurar SQL Database**  
   - Creaci√≥n de la base de datos SQL en Azure para la capa Gold (tablas agregadas y anal√≠ticas).
   - Soporte nativo con Power BI.

6. **Programar la ejecuci√≥n autom√°tica**  
   - Crear un **Trigger programado** en ADF (por ejemplo, a las 2:00 a.m. hora Colombia) para ejecutar el pipeline `pl_main` diariamente.  
   - Validar que los logs muestren el correcto procesamiento de archivos y generaci√≥n de tablas.

7. **Verificaci√≥n final**  
   - Consultar en Synapse las tablas de la capa Gold.  
   - Confirmar que los datasets de Bronze y Silver se generen correctamente en formato Delta dentro del Data Lake.  
   - (Opcional) Conectar Power BI a la base de datos SQL para visualizar los resultados.


## Uso del proyecto

Esta soluci√≥n fue desarrollada para **automatizar el proceso de integraci√≥n y an√°lisis de datos de ventas y clientes provenientes de m√∫ltiples tiendas**.  
El objetivo principal es **consolidar la informaci√≥n diaria de cada punto de venta en una √∫nica plataforma anal√≠tica en Azure**, garantizando la calidad, trazabilidad y disponibilidad de los datos para reportes y toma de decisiones.

En concreto, este pipeline permite:
- Recibir y procesar diariamente los archivos de ventas y clientes de cada tienda.  
- Unificar todos los datos en un modelo com√∫n (Common Data Model) con estructuras normalizadas.  
- Limpiar, validar y transformar los datos para crear datasets listos para an√°lisis.  
- Exponer los resultados finales en una **base de datos SQL** de **Azure Synapse Analytics**, conectada a herramientas de visualizaci√≥n como **Power BI**.  

Gracias a esta arquitectura, las empresas pueden:
- Monitorear las ventas y los clientes de todas las tiendas con **actualizaci√≥n diaria automatizada**.  
- Detectar inconsistencias o errores en los datos fuente antes de su an√°lisis.  
- Reducir tiempos manuales de carga y consolidaci√≥n de informaci√≥n.  
- Contar con una base s√≥lida para an√°lisis hist√≥ricos y reportes estrat√©gicos.


## Contribuciones

Este es un proyecto personal desarrollado con fines educativos y de demostraci√≥n profesional.  
Su objetivo principal es servir como una **muestra pr√°ctica de experiencia en Ingenier√≠a de Datos en Microsoft Azure**, abarcando todas las etapas del ciclo de vida de datos.

En este proyecto se emplearon las siguientes tecnolog√≠as y servicios:

- **Azure Data Factory (ADF)**
- **Azure Blob Storage**
- **Azure Data Lake Storage Gen2 (ADLS)**
- **Azure Synapse Analytics**
- **PySpark / Python**
- **Delta Lake**
- **SQL**
- **Git & GitHub**

> Aunque este proyecto utiliza **notebooks de Synapse Analytics** para el procesamiento de datos, la misma soluci√≥n podr√≠a haberse implementado con **Azure Databricks**.  
> Se eligi√≥ **Synapse** por motivos de **optimizaci√≥n de costos**, simplicidad operativa y su **conexi√≥n nativa con Power BI**, lo que facilita la exposici√≥n de los datos de la capa Gold para an√°lisis y visualizaci√≥n empresarial.

Este repositorio busca reflejar el dominio de herramientas y buenas pr√°cticas en **arquitecturas Lakehouse**, as√≠ como la capacidad de dise√±ar soluciones **automatizadas, escalables y orientadas a an√°lisis empresarial** dentro del ecosistema de Azure.

Actualmente no se aceptan contribuciones externas, pero cualquier sugerencia o comentario es bienvenido a trav√©s de los *issues* del repositorio o contactando directamente al autor.


## Licencia

Este proyecto fue desarrollado por **Alejandro Jos√© Eljadue Tarud** con fines educativos y de demostraci√≥n profesional.  
Su contenido puede ser utilizado libremente como referencia o inspiraci√≥n, siempre que se otorgue el cr√©dito correspondiente al autor.  

¬© 2025 Alejandro Jos√© Eljadue Tarud ‚Äì Todos los derechos reservados.
